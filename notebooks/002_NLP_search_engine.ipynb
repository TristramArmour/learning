{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine over Medium with Bag of WOrds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TristramArmour\\anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\TristramArmour\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>authors</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mental Note Vol. 24</td>\n",
       "      <td>Photo by Josh Riemer on Unsplash\\n\\nMerry Chri...</td>\n",
       "      <td>https://medium.com/invisible-illness/mental-no...</td>\n",
       "      <td>['Ryan Fan']</td>\n",
       "      <td>2020-12-26 03:38:10.479000+00:00</td>\n",
       "      <td>['Mental Health', 'Health', 'Psychology', 'Sci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Your Brain On Coronavirus</td>\n",
       "      <td>Your Brain On Coronavirus\\n\\nA guide to the cu...</td>\n",
       "      <td>https://medium.com/age-of-awareness/how-the-pa...</td>\n",
       "      <td>['Simon Spichak']</td>\n",
       "      <td>2020-09-23 22:10:17.126000+00:00</td>\n",
       "      <td>['Mental Health', 'Coronavirus', 'Science', 'P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mind Your Nose</td>\n",
       "      <td>Mind Your Nose\\n\\nHow smell training can chang...</td>\n",
       "      <td>https://medium.com/neodotlife/mind-your-nose-f...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2020-10-10 20:17:37.132000+00:00</td>\n",
       "      <td>['Biotechnology', 'Neuroscience', 'Brain', 'We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The 4 Purposes of Dreams</td>\n",
       "      <td>Passionate about the synergy between science a...</td>\n",
       "      <td>https://medium.com/science-for-real/the-4-purp...</td>\n",
       "      <td>['Eshan Samaranayake']</td>\n",
       "      <td>2020-12-21 16:05:19.524000+00:00</td>\n",
       "      <td>['Health', 'Neuroscience', 'Mental Health', 'P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Surviving a Rod Through the Head</td>\n",
       "      <td>You’ve heard of him, haven’t you? Phineas Gage...</td>\n",
       "      <td>https://medium.com/live-your-life-on-purpose/s...</td>\n",
       "      <td>['Rishav Sinha']</td>\n",
       "      <td>2020-02-26 00:01:01.576000+00:00</td>\n",
       "      <td>['Brain', 'Health', 'Development', 'Psychology...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              title  \\\n",
       "0               Mental Note Vol. 24   \n",
       "1         Your Brain On Coronavirus   \n",
       "2                    Mind Your Nose   \n",
       "3          The 4 Purposes of Dreams   \n",
       "4  Surviving a Rod Through the Head   \n",
       "\n",
       "                                                text  \\\n",
       "0  Photo by Josh Riemer on Unsplash\\n\\nMerry Chri...   \n",
       "1  Your Brain On Coronavirus\\n\\nA guide to the cu...   \n",
       "2  Mind Your Nose\\n\\nHow smell training can chang...   \n",
       "3  Passionate about the synergy between science a...   \n",
       "4  You’ve heard of him, haven’t you? Phineas Gage...   \n",
       "\n",
       "                                                 url                 authors  \\\n",
       "0  https://medium.com/invisible-illness/mental-no...            ['Ryan Fan']   \n",
       "1  https://medium.com/age-of-awareness/how-the-pa...       ['Simon Spichak']   \n",
       "2  https://medium.com/neodotlife/mind-your-nose-f...                      []   \n",
       "3  https://medium.com/science-for-real/the-4-purp...  ['Eshan Samaranayake']   \n",
       "4  https://medium.com/live-your-life-on-purpose/s...        ['Rishav Sinha']   \n",
       "\n",
       "                          timestamp  \\\n",
       "0  2020-12-26 03:38:10.479000+00:00   \n",
       "1  2020-09-23 22:10:17.126000+00:00   \n",
       "2  2020-10-10 20:17:37.132000+00:00   \n",
       "3  2020-12-21 16:05:19.524000+00:00   \n",
       "4  2020-02-26 00:01:01.576000+00:00   \n",
       "\n",
       "                                                tags  \n",
       "0  ['Mental Health', 'Health', 'Psychology', 'Sci...  \n",
       "1  ['Mental Health', 'Coronavirus', 'Science', 'P...  \n",
       "2  ['Biotechnology', 'Neuroscience', 'Brain', 'We...  \n",
       "3  ['Health', 'Neuroscience', 'Mental Health', 'P...  \n",
       "4  ['Brain', 'Health', 'Development', 'Psychology...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download dataset of Medium articles from \n",
    "# https://huggingface.co/datasets/fabiochiu/medium-articles\n",
    "df_articles = pd.read_csv(\n",
    "  hf_hub_download(\"fabiochiu/medium-articles\", repo_type=\"dataset\",\n",
    "                  filename=\"medium_articles.csv\")\n",
    ")\n",
    "\n",
    "# There are 192,368 articles in total, but let's keep only the first 10,000 to\n",
    "# make computations faster\n",
    "df_articles = df_articles[:10000]\n",
    "\n",
    "df_articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 32, ',': 52, 'we': 15, 'to': 30, 'for': 13, '.': 39, '’': 23, 'be': 13, 'that': 14, 'of': 23, '“': 12, 'the': 31, 'a': 16, '”': 11, 'i': 25, 'can': 16, 'it': 17, 's': 10}\n"
     ]
    }
   ],
   "source": [
    "# count the number of occurrences of each token in each text\n",
    "texts_lowercase = df_articles[\"text\"].str.lower()\n",
    "texts_lowercase_tokenized = texts_lowercase.apply(word_tokenize)\n",
    "token_counters = texts_lowercase_tokenized.apply(Counter).values.tolist()\n",
    "\n",
    "# show the tokens found in the first article with at least 10 occurrences\n",
    "print({token: n_occ for token, n_occ in token_counters[0].items() if n_occ >= 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the query\n",
    "query = \"data science nlp\"\n",
    "query_tokens = word_tokenize(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a matching score for each text with respect to the query. The score is\n",
    "# the number of times each token in the query can be found in a specific text.\n",
    "def get_scores(query_tokens, token_counters):\n",
    "  scores = []\n",
    "  for token_counter in token_counters:\n",
    "    matches = [token_counter[query_token] for query_token in query_tokens]\n",
    "    total_score = sum(matches)\n",
    "    scores.append(total_score)\n",
    "  return scores\n",
    "\n",
    "scores = get_scores(query_tokens, token_counters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [score = 186]: The Top Online Data Science Courses for 2019\n",
      "2 [score = 132]: How Much Do You Know About Your Data And Is Your Product Ready To Benefit From Data Science?\n",
      "3 [score = 122]: Under the Hood of K-Nearest Neighbors (KNN) and Popular Model Validation Techniques\n",
      "4 [score = 122]: Streaming Real-time data to AWS Elasticsearch using Kinesis Firehose\n",
      "5 [score = 120]: Financial Times Data Platform: From zero to hero\n",
      "6 [score = 118]: No data governance, no data intelligence!\n",
      "7 [score = 107]: Data Science for Everyone: Getting To Know Your Data — Part 1\n",
      "8 [score = 102]: Data Science, the Good, the Bad, and the… Future\n",
      "9 [score = 102]: A Layman’s Guide to Data Science: How to Become a (Good) Data Scientist\n",
      "10 [score = 98]: Data Science Minimum: 10 Essential Skills You Need to Know to Start Doing Data Science\n"
     ]
    }
   ],
   "source": [
    "# retrieve the top_n articles with the highest scores and show them\n",
    "def show_best_results(df_articles, scores, top_n=10):\n",
    "  best_indexes = np.argsort(scores)[::-1]\n",
    "  for position, idx in enumerate(best_indexes[:top_n]):\n",
    "    row = df_articles.iloc[idx]\n",
    "    title = row[\"title\"]\n",
    "    score = scores[idx]\n",
    "    print(f\"{position + 1} [score = {score}]: {title}\")\n",
    "\n",
    "show_best_results(df_articles, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [score = 589]: How to Make Your First $10,000 as a Freelance Writer\n",
      "2 [score = 583]: Russ Roberts and Tyler on COVID-19 (Ep. 90 — BONUS)\n",
      "3 [score = 526]: The Big Disruption\n",
      "4 [score = 461]: Sam Altman on Loving Community, Hating Coworking, and the Hunt for Talent (Ep. 61 — Live)\n",
      "5 [score = 394]: Paul Romer on a Culture of Science and Working Hard (Ep. 96)\n",
      "6 [score = 392]: Nicholas Bloom on Management, Productivity, and Scientific Progress (Ep. 102)\n",
      "7 [score = 349]: SXSW 2019 Ultimate Guide to the Panels, Popups and Parties\n",
      "8 [score = 341]: Glen Weyl on Fighting COVID-19 and the Role of the Academic Expert (Ep. 94 — BONUS)\n",
      "9 [score = 319]: The Top Online Data Science Courses for 2019\n",
      "10 [score = 308]: A Deep Conceptual Guide to Mutual Information\n"
     ]
    }
   ],
   "source": [
    "# try a different query\n",
    "query = \"how to learn data science\"\n",
    "query_tokens = word_tokenize(query)\n",
    "scores = get_scores(query_tokens, token_counters)\n",
    "show_best_results(df_articles, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this time the results are not as good as before. Why?\n",
    "\n",
    "That’s because the query contains tokens like “how” and “to”, which are very frequent in most of the articles in the dataset. As a consequence, the articles with the majority of these tokens are returned and tokens like “data” and “science” have less influence on the results.\n",
    "\n",
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TristramArmour\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "english_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'’': 23, '“': 12, 'us': 6, '”': 11, 'life': 6}\n"
     ]
    }
   ],
   "source": [
    "# count the number of occurrences of each token in each text\n",
    "texts_lowercase = df_articles[\"text\"].str.lower()\n",
    "texts_lowercase_tokenized = texts_lowercase.apply(word_tokenize)\n",
    "texts_lowercase_tokenized_no_sw = texts_lowercase_tokenized.apply(\n",
    "  lambda token_list: [token for token in token_list\n",
    "                      if token not in english_stopwords and\n",
    "                      token not in string.punctuation]\n",
    ")\n",
    "token_counters = texts_lowercase_tokenized_no_sw.apply(Counter).values.tolist()\n",
    "\n",
    "# show the tokens found in the first article with at least 6 occurrences\n",
    "print({token: n_occ for token, n_occ in token_counters[0].items() if n_occ >= 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized query without stopwords: ['learn', 'data', 'science']\n",
      "\n",
      "1 [score = 200]: The Top Online Data Science Courses for 2019\n",
      "2 [score = 132]: How Much Do You Know About Your Data And Is Your Product Ready To Benefit From Data Science?\n",
      "3 [score = 124]: Under the Hood of K-Nearest Neighbors (KNN) and Popular Model Validation Techniques\n",
      "4 [score = 123]: Streaming Real-time data to AWS Elasticsearch using Kinesis Firehose\n",
      "5 [score = 120]: Financial Times Data Platform: From zero to hero\n",
      "6 [score = 119]: No data governance, no data intelligence!\n",
      "7 [score = 107]: Data Science for Everyone: Getting To Know Your Data — Part 1\n",
      "8 [score = 107]: A Layman’s Guide to Data Science: How to Become a (Good) Data Scientist\n",
      "9 [score = 104]: Data Science, the Good, the Bad, and the… Future\n",
      "10 [score = 99]: Data Science Minimum: 10 Essential Skills You Need to Know to Start Doing Data Science\n"
     ]
    }
   ],
   "source": [
    "# tokenize the query and remove stopwords\n",
    "query = \"how to learn data science\"\n",
    "query_tokens = word_tokenize(query)\n",
    "query_tokens_no_sw = [token for token in query_tokens\n",
    "                      if token not in english_stopwords and\n",
    "                      token not in string.punctuation]\n",
    "print(f\"Tokenized query without stopwords: {query_tokens_no_sw}\")\n",
    "print()\n",
    "\n",
    "# show best results\n",
    "scores = get_scores(query_tokens, token_counters)\n",
    "show_best_results(df_articles, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
