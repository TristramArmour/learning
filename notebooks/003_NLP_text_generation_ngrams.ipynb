{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download books from Project Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\TristramArmour\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list all corpora from project Gutenberg in NLTK\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set him before me , let me see his face Cassi . Fellow , come from the throng , look vpon Caesar Caes . What sayst thou to me now ? Speak once againe , Sooth . Beware the Ides of March Caes . He is a Dreamer\n"
     ]
    }
   ],
   "source": [
    "# list of Shakespeare corpora\n",
    "shakespeare_corpora = [\n",
    "    \"shakespeare-caesar.txt\",\n",
    "    \"shakespeare-hamlet.txt\",\n",
    "    \"shakespeare-macbeth.txt\"\n",
    "]\n",
    "\n",
    "# get all corpora\n",
    "corpora = {\n",
    "    corpus_name: nltk.corpus.gutenberg.words(corpus_name)\n",
    "    for corpus_name in shakespeare_corpora\n",
    "}\n",
    "\n",
    "# print some sentences from a corpus...\n",
    "some_n_tokens = corpora[\"shakespeare-caesar.txt\"][1002:1050]\n",
    "print(\" \".join(some_n_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Occurrencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 37), ('caesars', 1), ('your', 5), ('their', 3), ('brutus', 1), ('that', 2), ('seuerall', 1), ('qualitie', 1), ('bondage', 1), ('power', 1)]\n"
     ]
    }
   ],
   "source": [
    "# count how many times a specific token is right after another specific token\n",
    "# in the corpora\n",
    "\n",
    "# example: from the text \"the dog is under the table.\", we want to obtain the dictionary\n",
    "# {\n",
    "#  \"the\": { \"dog\": 1, \"table\": 1 },\n",
    "#  \"dog\": { \"is\": 1 },\n",
    "#  \"is\": { \"under\": 1 },\n",
    "#  \"under\": { \"the\": 1 },\n",
    "#  \"table\": { \".\": 1 }\n",
    "# }\n",
    "\n",
    "# from_token_to_next_token_counts = { token: { next_token: num_of_occurrencies } }\n",
    "from_token_to_next_token_counts = defaultdict(dict)\n",
    "\n",
    "for corpus in corpora.values():\n",
    "  for i in range(len(corpus) - 1):\n",
    "    token = corpus[i].lower()\n",
    "    next_token = corpus[i + 1].lower()\n",
    "    if next_token not in from_token_to_next_token_counts[token]:\n",
    "      from_token_to_next_token_counts[token][next_token] = 0\n",
    "    from_token_to_next_token_counts[token][next_token] += 1\n",
    "\n",
    "# print 10 examples of tokens that followed the token \"from\" in the corpora, along\n",
    "# with their counts of occurrences\n",
    "print(list(from_token_to_next_token_counts[\"from\"].items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 0.19170984455958548), ('caesars', 0.0051813471502590676), ('your', 0.025906735751295335), ('their', 0.015544041450777202), ('brutus', 0.0051813471502590676), ('that', 0.010362694300518135), ('seuerall', 0.0051813471502590676), ('qualitie', 0.0051813471502590676), ('bondage', 0.0051813471502590676), ('power', 0.0051813471502590676)]\n"
     ]
    }
   ],
   "source": [
    "# transform occurrencies into probabilities\n",
    "\n",
    "# example: from the text \"the dog is under the table.\", we want to obtain the dictionary\n",
    "# {\n",
    "#  \"the\": { \"dog\": 0.5, \"table\": 0.5 },\n",
    "#  \"dog\": { \"is\": 1 },\n",
    "#  \"is\": { \"under\": 1 },\n",
    "#  \"under\": { \"the\": 1 },\n",
    "#  \"table\": { \".\": 1 }\n",
    "# }\n",
    "\n",
    "# from_token_to_next_token_probs = { token: { next_token: probability } }\n",
    "from_token_to_next_token_probs = {}\n",
    "\n",
    "for token, d_token in from_token_to_next_token_counts.items():\n",
    "  sum_of_counts_for_token = sum(d_token.values())\n",
    "  from_token_to_next_token_probs[token] = {\n",
    "      next_token: count / sum_of_counts_for_token\n",
    "      for next_token, count\n",
    "      in d_token.items()\n",
    "  }\n",
    "\n",
    "# print 10 examples of tokens that followed the token \"from\" in the corpora, along\n",
    "# with their probabilities\n",
    "print(list(from_token_to_next_token_probs[\"from\"].items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiffe\n"
     ]
    }
   ],
   "source": [
    "# sample the next token according to the computed probabilities\n",
    "def sample_next_token(token, from_token_to_next_token_probs):\n",
    "  next_tokens, next_tokens_probs = list(zip(*from_token_to_next_token_probs[token].items()))\n",
    "  next_token_sampled = np.random.choice(next_tokens, size=1, p=next_tokens_probs)[0]\n",
    "  return next_token_sampled\n",
    "\n",
    "print(sample_next_token(\"from\", from_token_to_next_token_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from her coronet weeds : tis an hundred ducates a beere - day , i am bent for them againe to heart : it offends mee , dar ' d into the age dotes on his name of them i know your nature , but change to england , no other\n"
     ]
    }
   ],
   "source": [
    "# repeatedly sample tokens to generate long text\n",
    "def generate_text_from_token(token, from_token_to_next_token_probs, n_words_to_generate):\n",
    "  text = token\n",
    "  for _ in range(n_words_to_generate):\n",
    "    next_token = sample_next_token(token, from_token_to_next_token_probs)\n",
    "    text += \" \" + next_token\n",
    "    token = next_token\n",
    "  return text\n",
    "\n",
    "first_token = \"from\"\n",
    "n_words_to_generate = 50\n",
    "generated_text = generate_text_from_token(first_token, from_token_to_next_token_probs, n_words_to_generate)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generally we should not use more than 3 gram, probably 2 gram. Even with 3 books of text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193\n",
      "37\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# merge all the corpora in a single string\n",
    "all_corpora_tokens = corpora[\"shakespeare-caesar.txt\"] + corpora[\"shakespeare-hamlet.txt\"] + corpora[\"shakespeare-macbeth.txt\"]\n",
    "all_corpora_tokens = [token.lower() for token in all_corpora_tokens]\n",
    "all_corpora_text = \" \".join(all_corpora_tokens)\n",
    "\n",
    "# see how many specific 1-grams, 2-grams, 3-grams can be found in the corpus\n",
    "print(all_corpora_text.count(\"from \")) # 1-grams\n",
    "print(all_corpora_text.count(\"from the \")) # 2-grams\n",
    "print(all_corpora_text.count(\"from the streets \")) # 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
