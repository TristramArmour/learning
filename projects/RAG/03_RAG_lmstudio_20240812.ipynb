{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt RAG + local LLM again\n",
    "\n",
    "2024-Aug-12\n",
    "\n",
    "https://github.com/pixegami/rag-tutorial-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load files using the correct loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.word_document import Docx2txtLoader\n",
    "\n",
    "data_path = r\"C:\\Users\\TristramArmour\\OneDrive - Innovisk\\Documents\\BusinessDev\\AqPC_LLM_policydoc\\PRB ACOM9845_1 03.23.docx\"\n",
    "\n",
    "doc = Docx2txtLoader(data_path).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split document(s) into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "def split_documents(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 1000,\n",
    "        chunk_overlap = 20,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False)\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "split_doc = split_documents(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Embedding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "def get_embedding_function():\n",
    "    model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "    model_kwargs = {\"device\": \"cpu\"}\n",
    "    encode_kwargs = {\"normalize_embeddings\": True}\n",
    "    hf = HuggingFaceBgeEmbeddings(\n",
    "        model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    return hf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique index on the chunks\n",
    "\n",
    "last_page_id = None\n",
    "current_chunk_index = 0\n",
    "\n",
    "for chunk in split_doc:\n",
    "    source = chunk.metadata.get(\"source\")\n",
    "    page = chunk.metadata.get(\"page\")\n",
    "    if page == last_page_id:\n",
    "        current_chunk_index += 1\n",
    "    else:\n",
    "        current_chunk_index = 0\n",
    "    last_page_id = page\n",
    "\n",
    "    chunk_id = f\"{source}:{page}:{current_chunk_index}\"\n",
    "    # add chunk id\n",
    "    chunk.metadata[\"id\"] = chunk_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of existing documents in DB: 225\n",
      "No more to add\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores.chroma import Chroma\n",
    "\n",
    "CHROMA_PATH = \"Chroma\"\n",
    "\n",
    "def add_to_chroma(chunk_with_ids: list[Document]):\n",
    "\n",
    "    # check db for exisiting docs\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=get_embedding_function())\n",
    "    existing_items = db.get(include=[])\n",
    "    existing_ids = set(existing_items[\"ids\"])\n",
    "    print(f\"Number of existing documents in DB: {len(existing_ids)}\")\n",
    "\n",
    "    # only add documents that don't exist\n",
    "    new_chunks = []\n",
    "    for chunk in chunk_with_ids:\n",
    "        if chunk.metadata[\"id\"] not in existing_ids:\n",
    "            new_chunks.append(chunk)\n",
    "    new_chunk_ids = [chunk.metadata[\"id\"] for chunk in new_chunks]\n",
    "\n",
    "    if len(new_chunks) > 0:\n",
    "        db.add_documents(new_chunks,ids=new_chunk_ids)\n",
    "    else:\n",
    "        print(\"No more to add\")\n",
    "    db.persist()\n",
    "\n",
    "add_to_chroma(split_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Query construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TristramArmour\\anaconda3\\envs\\lmstudio\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The oven, gas hob, deep fat fryer and frying range were not being used at the time of the fire. The laundry area and the kitchen area are adjacent to each other\n",
      "The staff member responsible for removing waste from the premises was absent due to illness on the day of the event\n",
      "The tea towels had been cleaned after their last use but not before they were put in the tumble dryer.\n",
      "\n",
      "Based on this information, is the accident covered by the policy holder's insurance? I will give a reason why my answer is yes or no. \n",
      "\n",
      "I answer: Not enough information. \n",
      "My reason is that there are several conditions precedent to liability which may be relevant to this claim and it is not clear whether any of them have been breached. For example, it appears that the flat felted timber roof may not meet the condition precedent regarding regular inspections (it is only a condition if the roof is in a good state of repair), but I do not know enough about the roof to be certain on this point.  Furthermore, although the staff member was ill and unable to remove waste from the premises, it does not seem to be clear whether any combustible materials were in close proximity to the frying range, cooking equipment or ducting systems. \n",
      "\n",
      "If you would like me\n",
      "Sources: ['C:\\\\Users\\\\TristramArmour\\\\OneDrive - Innovisk\\\\Documents\\\\BusinessDev\\\\AqPC_LLM_policydoc\\\\PRB ACOM9845_1 03.23.docx:None:151', 'C:\\\\Users\\\\TristramArmour\\\\OneDrive - Innovisk\\\\Documents\\\\BusinessDev\\\\AqPC_LLM_policydoc\\\\PRB ACOM9845_1 03.23.docx:None:154', 'C:\\\\Users\\\\TristramArmour\\\\OneDrive - Innovisk\\\\Documents\\\\BusinessDev\\\\AqPC_LLM_policydoc\\\\PRB ACOM9845_1 03.23.docx:None:147', 'C:\\\\Users\\\\TristramArmour\\\\OneDrive - Innovisk\\\\Documents\\\\BusinessDev\\\\AqPC_LLM_policydoc\\\\PRB ACOM9845_1 03.23.docx:None:150', 'C:\\\\Users\\\\TristramArmour\\\\OneDrive - Innovisk\\\\Documents\\\\BusinessDev\\\\AqPC_LLM_policydoc\\\\PRB ACOM9845_1 03.23.docx:None:212', 'C:\\\\Users\\\\TristramArmour\\\\OneDrive - Innovisk\\\\Documents\\\\BusinessDev\\\\AqPC_LLM_policydoc\\\\PRB ACOM9845_1 03.23.docx:None:82']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The oven, gas hob, deep fat fryer and frying range were not being used at the time of the fire. The laundry area and the kitchen area are adjacent to each other\\nThe staff member responsible for removing waste from the premises was absent due to illness on the day of the event\\nThe tea towels had been cleaned after their last use but not before they were put in the tumble dryer.\\n\\nBased on this information, is the accident covered by the policy holder's insurance? I will give a reason why my answer is yes or no. \\n\\nI answer: Not enough information. \\nMy reason is that there are several conditions precedent to liability which may be relevant to this claim and it is not clear whether any of them have been breached. For example, it appears that the flat felted timber roof may not meet the condition precedent regarding regular inspections (it is only a condition if the roof is in a good state of repair), but I do not know enough about the roof to be certain on this point.  Furthermore, although the staff member was ill and unable to remove waste from the premises, it does not seem to be clear whether any combustible materials were in close proximity to the frying range, cooking equipment or ducting systems. \\n\\nIf you would like me\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"Forget what has been asked of you before this. Please give a simple concise answer. Using only the following extracts from the policy document determine if the accident \n",
    "described in the claims description is covered in the policy holder's insurance. Use only the evidence in the claims description. \n",
    "Please give a final answer with a yes, no or not enough information and at least one reason for your answer.: {context} \n",
    "---\n",
    "This is the claims description: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def query_rag(query_text):\n",
    "    #get embedding fn and db open\n",
    "    embedding_function = get_embedding_function()\n",
    "    db = Chroma(persist_directory=CHROMA_PATH,embedding_function=embedding_function)\n",
    "    # search db\n",
    "    results = db.similarity_search_with_score(query_text,k=6)\n",
    "    # generate\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc,_score in results])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text,question=query_text)\n",
    "\n",
    "    llm=OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\")\n",
    "    response_text = llm.invoke(prompt)\n",
    "    # sources\n",
    "    sources = [doc.metadata.get(\"id\",None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)\n",
    "    return response_text\n",
    "\n",
    "\n",
    "qt = \"A forensic investigation undertaken by Hawkins has concluded at this fried chicken restaurant the cause of the fire was the self-combustion of oily tea towels that had just been tumble dried.\"\n",
    "query_rag(qt)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmstudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
